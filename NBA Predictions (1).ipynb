{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data from 1984-2018\n",
    "historical = pd.read_csv('historical-data.csv')\n",
    "historical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stats for 2019-20\n",
    "this_year = pd.read_csv('2020 stats.csv')\n",
    "this_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= ['team_wins','per', 'fgm', 'fg_perc', '3pm', '3p_perc', 'ft_perc', 'orb', 'drb', 'trb', 'ast', 'stl', 'blk', 'pts', 'ts', 'trb_perc', 'ast_perc', 'usg_perc', 'ws', 'ws_per_48', 'bpm', 'vorp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = historical[features].to_numpy()\n",
    "y_data = historical[['vote_share']].to_numpy()\n",
    "#y_data = y_data.reshape(y_data.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(x_data, y_data.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(rf.feature_importances_)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in sorted_indices:\n",
    "    print(f\"{features[index]}: {rf.feature_importances_[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = SelectKBest(mutual_info_regression, k=10)\n",
    "best_features = kbest.fit_transform(x_data,y_data.ravel())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = np.argsort(kbest.scores_)[::-1]\n",
    "for index in sorted_indices:\n",
    "    print(f\"{features[index]}: {kbest.scores_[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = historical[features]\n",
    "y = historical['vote_share']\n",
    "corrmat = historical.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(historical[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = historical[features]\n",
    "feat['vote_share'] = historical['vote_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = feat[features]\n",
    "y = feat['vote_share']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = feat.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(feat[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, normalize, StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns = ['team_wins','per', 'trb', 'ast', 'stl', 'blk', 'pts', 'ts', 'usg_perc', 'ws', 'vorp']\n",
    "label_columns = ['vote_share']\n",
    "\n",
    "season = 2018\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 200, max_depth = 5)\n",
    "poly_fit = PolynomialFeatures(degree=2, interaction_only = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = historical.loc[historical.season_start!=season]\n",
    "validation_data = historical.loc[historical.season_start==season]\n",
    "\n",
    "train_x = train_data[train_columns].to_numpy()\n",
    "train_y = train_data[label_columns].to_numpy()\n",
    "train_y = train_y.reshape(train_y.shape[0], )\n",
    "#train_x = poly_fit.fit_transform(train_x)\n",
    "\n",
    "\n",
    "validation_x = validation_data[train_columns].to_numpy()\n",
    "#validation_x = poly_fit.fit_transform(validation_x)\n",
    "validation_y = validation_data[label_columns].to_numpy()\n",
    "validation_y = validation_y.reshape(validation_y.shape[0], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y)\n",
    "predict_y_RF = model.predict(validation_x)\n",
    "predict_y_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, verbose=1, max_depth = 10)\n",
    "model1.fit(train_x, train_y)\n",
    "predict_y = model1.predict(validation_x)\n",
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(max_iter = 350)\n",
    "nn.fit(train_x, train_y)\n",
    "preds = nn.predict(validation_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBRegressor(objective = 'reg:squarederror', n_estimators = 200, max_depth = 5, learning_rate = 0.001, alpha = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= xg.predict(validation_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_predictions = pd.DataFrame(columns = ['Season', 'Random Forest', 'GBR', 'Bagging', 'AdaBoost', 'Real Vote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions (poly_fit = None, scaler = None):\n",
    "    global yearly_predictions\n",
    "    #trial_results = pd.DataFrame(columns = ['Season', 'Random Forest', 'GBR', 'Real Vote'])\n",
    "    unique_seasons = historical.season_start.unique()\n",
    "    train_columns = ['team_wins','per', 'trb', 'ast', 'stl', 'blk', 'pts', 'ts', 'usg_perc', 'ws', 'vorp']\n",
    "    label_columns = ['vote_share']\n",
    "\n",
    "    \n",
    "    for seasons in unique_seasons:\n",
    "        #print(seasons)\n",
    "        RF = RandomForestRegressor(n_estimators=200, n_jobs=-1)\n",
    "        GBR = GradientBoostingRegressor(learning_rate = 0.01, n_estimators = 200)\n",
    "        Bagging = BaggingRegressor(n_estimators=200, n_jobs = -1)\n",
    "        Ada = AdaBoostRegressor(n_estimators = 200, learning_rate = 0.3)\n",
    "        train_data = historical.loc[historical.season_start!=seasons]\n",
    "        validation_data = historical.loc[historical.season_start==seasons]\n",
    "        \n",
    "        train_x = train_data[train_columns].to_numpy()\n",
    "        train_y = train_data[label_columns].to_numpy()\n",
    "        train_y = train_y.reshape(train_y.shape[0], )\n",
    "\n",
    "        validation_x = validation_data[train_columns].to_numpy()\n",
    "        validation_y = validation_data[label_columns].to_numpy()\n",
    "        validation_y = validation_y.reshape(validation_y.shape[0], )\n",
    "        \n",
    "        if poly_fit is not None:\n",
    "            train_x = poly_fit.fit_transform(train_x)\n",
    "            validation_x = poly_fit.fit_transform(validation_x)\n",
    "            \n",
    "        if scaler is not None:\n",
    "            train_x = scaler.fit_transform(train_x)\n",
    "            validation_x = scaler.fit_transform(validation_x)\n",
    "        \n",
    "        RF.fit(train_x, train_y)\n",
    "        RF_preds = RF.predict(validation_x)\n",
    "        \n",
    "        GBR.fit(train_x, train_y)\n",
    "        GBR_preds = GBR.predict(validation_x)\n",
    "        \n",
    "        Bagging.fit(train_x, train_y)\n",
    "        Bagging_preds = Bagging.predict(validation_x)\n",
    "        \n",
    "        Ada.fit(train_x, train_y)\n",
    "        Ada_preds = Ada.predict(validation_x)\n",
    "        \n",
    "        \n",
    "        for i in range(len(validation_x)):\n",
    "            a = np.array([seasons, RF_preds[i], GBR_preds[i], Bagging_preds[i], Ada_preds[i], validation_y[i]])\n",
    "            #print(validation_y[i])\n",
    "            a= pd.Series(a, index=['Season', 'Random Forest', 'GBR', 'Bagging', 'AdaBoost', 'Real Vote'])\n",
    "            yearly_predictions = yearly_predictions.append(a, ignore_index = True )         \n",
    "    return yearly_predictions\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions(PolynomialFeatures(3), MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_predictions.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_excel('rf_gbr_bagging_ada.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = pd.read_csv('2020 stats.csv')\n",
    "current.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = np.zeros(10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_year(model, poly_fit, train, test, scalar=None, should_print =True):\n",
    "    global raw_predictions\n",
    "    #raw_predictions = []\n",
    "    train_columns = ['team_wins','per', 'trb', 'ast', 'stl', 'blk', 'pts', 'ts', 'usg_perc', 'ws', 'vorp']\n",
    "    label_columns = ['vote_share']\n",
    "    \n",
    "    train_x = train[train_columns].to_numpy()\n",
    "    train_y = train[label_columns].to_numpy()\n",
    "    \n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    train_y = train_y.reshape(train_y.shape[0], )\n",
    "    \n",
    "    test_x = test[train_columns].to_numpy()\n",
    "    test_x = np.nan_to_num(test_x)\n",
    "    \n",
    "    \n",
    "    if poly_fit is not None:\n",
    "        train_x = poly_fit.fit_transform(train_x)\n",
    "        test_x = poly_fit.fit_transform(test_x)\n",
    "    \n",
    "    if scaler is not None:\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        test_x = scaler.fit_transform(test_x)\n",
    "        \n",
    "    model.fit(train_x, train_y)\n",
    "    predict_y = model.predict(test_x)\n",
    "    print(predict_y.shape)\n",
    "    raw_predictions = np.add(raw_predictions, predict_y)\n",
    "    sorted_indices = np.argsort(predict_y)[::-1]\n",
    "    predictions = predict_y[sorted_indices]\n",
    "\n",
    "    \n",
    "    formatted_preds = []\n",
    "    if should_print:\n",
    "        print(f\"Predictions:\")\n",
    "    for i in range(10):\n",
    "        if predictions[i] < 0:\n",
    "            break\n",
    "        if should_print:\n",
    "            print(f\"{i+1}. {current.iloc[sorted_indices[i]].Player}: {predictions[i]}\")\n",
    "        formatted_preds.append((current.iloc[sorted_indices[i]].Player, predictions[i]))\n",
    "    return formatted_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = RandomForestRegressor(n_estimators =100, n_jobs=-1),\n",
    "    poly_fit = PolynomialFeatures(2, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = RandomForestRegressor(n_estimators =200, n_jobs=-1),\n",
    "    poly_fit = PolynomialFeatures(2, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = RandomForestRegressor(n_estimators =100, n_jobs=-1),\n",
    "    poly_fit = PolynomialFeatures(3, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = RandomForestRegressor(n_estimators =200, n_jobs=-1),\n",
    "    poly_fit = PolynomialFeatures(3, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = GradientBoostingRegressor(n_estimators =100,alpha= 0.9, max_depth = 7, learning_rate=0.1),\n",
    "    poly_fit = PolynomialFeatures(2, interaction_only = True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = GradientBoostingRegressor(n_estimators =300,alpha= 0.9, max_depth = 5),\n",
    "    poly_fit = PolynomialFeatures(2, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = GradientBoostingRegressor(n_estimators =100,alpha= 0.9, max_depth = 5),\n",
    "    poly_fit = PolynomialFeatures(3, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = GradientBoostingRegressor(n_estimators =300,alpha= 0.9, max_depth = 5),\n",
    "    poly_fit = PolynomialFeatures(3, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = AdaBoostRegressor(n_estimators = 50, learning_rate= 1.0),\n",
    "    poly_fit = PolynomialFeatures(2, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = AdaBoostRegressor(n_estimators = 100, learning_rate= 1.0),\n",
    "    poly_fit = PolynomialFeatures(2, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = AdaBoostRegressor(n_estimators = 100, learning_rate= 0.5),\n",
    "    poly_fit = PolynomialFeatures(2, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_year(\n",
    "    model = AdaBoostRegressor(n_estimators = 100, learning_rate= 1.0),\n",
    "    poly_fit = PolynomialFeatures(3, interaction_only=True),\n",
    "    train = historical,\n",
    "    test = current,\n",
    "    scalar = MinMaxScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions /=12\n",
    "raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_print = 1\n",
    "sorted_indices = np.argsort(raw_predictions)[::-1]\n",
    "predictions = raw_predictions[sorted_indices]\n",
    "\n",
    "    \n",
    "formatted_preds = []\n",
    "if should_print:\n",
    "    print(f\"Predictions:\")\n",
    "for i in range(10):\n",
    "    if predictions[i] < 0:\n",
    "        break\n",
    "    if should_print:\n",
    "        print(f\"{i+1}. {current.iloc[sorted_indices[i]].Player}: {predictions[i]}\")\n",
    "    formatted_preds.append((current.iloc[sorted_indices[i]].Player, predictions[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [item[0].split(\" \")[0][0] + \". \" + item[0].split(\" \")[1] for item in formatted_preds]\n",
    "names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "ax.bar([item[0] for item in formatted_preds], [item[1] for item in formatted_preds])\n",
    "ax.set_title(\"Average vote share of MVP Candidates based on 12 models\", size=18)\n",
    "ax.set_ylabel(\"Vote Share\", size=14)\n",
    "_ = ax.set_xticklabels(labels=[item[0] for item in formatted_preds], rotation=45, ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
